
[global]
# A frequent task is to extract files to process for batch processing, and build lists for e.g. SLURM to process in array jobs that can take weeks to run. 
# This tells DataCurator to do that for you, build in and output lists, that match each other, and can be used to run batch jobs.
# `sort` and unique are used to remove duplicates, and `shared_list_to_file` is used to build the lists.
act_on_success=true
file_lists = [{name="table", aggregator=[["filepath",
                                          "sort",
                                          "unique",
                                          "shared_list_to_file"]]},
              {name="out", aggregator=[[["change_path", "/tmp/output"],
                                         "filepath",
                                         "sort",
                                         "unique",
                                         "shared_list_to_file"]]}
              ]
inputdirectory = "testdir"
[any]
all=true
conditions = ["is_csv_file"]
actions=[["add_to_file_list", "table"], ["add_to_file_list", "out"]]

#[[["change_path", "/tmp/output"],
#                                         "filepath",
#                                         "sort",
#                                         "unique",
#                                         "shared_list_to_file"]]}
#              ]
# For a list of files, change the prefix to /tmp/output, strip the filename so only the path remains, sort, then remove duplicates
# the result is saved in a file#
# E.g. /a/b/c/d.csv and /a/b/e/q.csv
# Would become /tmp/output/a/c and /tmp/output/e 